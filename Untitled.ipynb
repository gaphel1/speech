{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN=r'C:\\Users\\Turing Gaphel\\Documents\\3new\\dataset\\train'\n",
    "PATH_TEST=r'C:\\Users\\Turing Gaphel\\Documents\\3new\\dataset\\test'\n",
    "BATCH_SIZE=100\n",
    "ITERATIONS=500\n",
    "ITERATIONS_TEST=10\n",
    "EVAL_EVERY=5\n",
    "HEIGHT=20\n",
    "WIDTH=44\n",
    "NUM_LABELS=0\n",
    "LEARNING_RATE=1E-4\n",
    "LOGDIR='log/'\n",
    "TEST_LOGDIR='log_test'\n",
    "LABEL_TO_INDEX_MAP={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(path):\n",
    "    labels=os.listdir(path)\n",
    "    index =0\n",
    "    for label in labels:\n",
    "        LABEL_TO_INDEX_MAP[label]=index\n",
    "        index+=1\n",
    "    global NUM_LABELS\n",
    "    NUM_LABELS=len(LABEL_TO_INDEX_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(label):\n",
    "    encoding = [0] * len(LABEL_TO_INDEX_MAP)\n",
    "    encoding[LABEL_TO_INDEX_MAP[label]]=1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(wave_path,PAD_WIDTH=WIDTH):\n",
    "    wave, sr = librosa.load(wave_path,mono=True)\n",
    "    mfccs = librosa.feature.mfcc(y=wave,sr=sr,n_mfcc=HEIGHT)\n",
    "    mfccs = np.pad(mfccs,((0,0),(0,PAD_WIDTH-len(mfccs[0]))),mode='constant')\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size,path):\n",
    "    X = []\n",
    "    Y=[]\n",
    "    random.seed(5896)\n",
    "    path=os.path.join(path,'*','*.wav')\n",
    "    waves=gfile.Glob(path)\n",
    "    while True:\n",
    "        random.shuffle(waves)\n",
    "        for wave_path in waves:\n",
    "            _,label=os.path.split(os.path.dirname(wave_path))\n",
    "            X.append(get_mfcc(wave_path))\n",
    "            Y.append(one_hot_encoding(label))\n",
    "            if (len(X)==batch_size):\n",
    "                yield X,Y\n",
    "                X=[]\n",
    "                Y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input,dropout):\n",
    "    with tf.name_scope('Conv1'):\n",
    "        input_4D=tf.reshape(input,[-1,HEIGHT,WIDTH,1])\n",
    "        w1=tf.Variable(tf.truncated_normal([12,8,1,44],stddev=0.01),name=\"W\")\n",
    "        b1=tf.Variable(tf.zeros([44]),name='B')\n",
    "        conv1=tf.nn.conv2d(input_4D,w1,strides=[1,1,1,1],padding='SAME')\n",
    "        act1=tf.nn.relu(conv1+b1)\n",
    "        drop1=tf.nn.dropout(act1,dropout)\n",
    "        max_pool1=tf.nn.max_pool(drop1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "        tf.summary.histogram('weight',w1)\n",
    "        tf.summary.histogram('biases',b1)\n",
    "        tf.summary.histogram('activations',act1)\n",
    "        tf.summary.histogram('dropouts',drop1)\n",
    "    \n",
    "    with tf.name_scope('Conv2'):\n",
    "        w2=tf.Variable(tf.truncated_normal([12,16,1,44],stddev=0.01),name=\"W\")\n",
    "        b2=tf.Variable(tf.zeros([44]),name='B')\n",
    "        conv2=tf.nn.conv2d(input_4D,w2,strides=[1,1,1,1],padding='SAME')\n",
    "        act2=tf.nn.relu(conv2+b2)\n",
    "        drop2=tf.nn.dropout(act2,dropout)\n",
    "        max_pool2=tf.nn.max_pool(drop2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "        tf.summary.histogram('weight',w2)\n",
    "        tf.summary.histogram('biases',b2)\n",
    "        tf.summary.histogram('activations',act2)\n",
    "        tf.summary.histogram('dropouts',drop2)\n",
    "        \n",
    "    with tf.name_scope('Conv3'):\n",
    "        w3=tf.Variable(tf.truncated_normal([6,4,44,44],stddev=0.01),name=\"W\")\n",
    "        b3=tf.Variable(tf.zeros([44]),name='B')\n",
    "        conv3=tf.nn.conv2d(max_pool1,w3,strides=[1,1,1,1],padding='SAME')\n",
    "        act3=tf.nn.relu(conv3+b3)\n",
    "        drop3=tf.nn.dropout(act3,dropout)\n",
    "        # max_pool1=tf.nn.max_pool(drop1,ksize=[1,2,2,1],strides=[1,2,2,1]),padding='SAME'\n",
    "        tf.summary.histogram('weight',w3)\n",
    "        tf.summary.histogram('biases',b3)\n",
    "        tf.summary.histogram('activations',act3)\n",
    "        tf.summary.histogram('dropouts',drop3)\n",
    "        \n",
    "        conv_shape=drop3.get_shape()\n",
    "        count =int(conv_shape[1]*conv_shape[2]*conv_shape[3])\n",
    "        flat_output=tf.reshape(drop3,[-1,count])\n",
    "\n",
    "        with tf.name_scope('FC'):\n",
    "            w4=tf.Variable(tf.truncated_normal([count,NUM_LABELS],stddev=0.01))\n",
    "            b4=tf.Variable(tf.zeros([NUM_LABELS]))\n",
    "            fc=tf.add(tf.matmul(flat_output,w4),b4)\n",
    "            tf.summary.histogram('weight',w4)\n",
    "            tf.summary.histogram('biases',b4)\n",
    "            \n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tf.reset_default_graph()\n",
    "    sess=tf.Session()\n",
    "\n",
    "    x=tf.placeholder(tf.float32,shape=[None,HEIGHT,WIDTH], name='input')\n",
    "\n",
    "    y=tf.placeholder(tf.float32,shape=[None,NUM_LABELS], name='label')\n",
    "\n",
    "    dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "    logits=get_model(x,dropout)\n",
    "\n",
    "    #loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "        #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "        # might have erroe\n",
    "        tf.summary.scalar('loss',loss)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        #train_step =tf.train.AdamOptmizer(LEARNING_RATE).minimize(loss)\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        predicted = tf.argmax(logits,1)\n",
    "        truth =tf.argmax(y,1)\n",
    "        correct_prediction=tf.equal(predicted,truth)\n",
    "        accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        confusion_matrix=tf.confusion_matrix(truth, predicted, num_classes=NUM_LABELS)\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "        \n",
    "    summ=tf.summary.merge_all()\n",
    "    saver=tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer=tf.summary.FileWriter(LOGDIR)\n",
    "    writer.add_graph(sess.graph)\n",
    "    test_writer=tf.summary.FileWriter(TEST_LOGDIR)\n",
    "\n",
    "    #traing\n",
    "    print('start traininig\\n')\n",
    "    batch=get_batch(BATCH_SIZE,PATH_TRAIN)\n",
    "    start_time=time.time()\n",
    "    for i in range(1,ITERATIONS + 1):\n",
    "        X,Y =next(batch)\n",
    "        if i % EVAL_EVERY ==0:\n",
    "            #error\n",
    "            [train_accuracy,train_loss,s]=sess.run([accuracy,loss,summ],feed_dict={x:X,y:Y,dropout:0.5})\n",
    "            acc_and_loss = [i,train_loss,train_accuracy *100]\n",
    "            print('iteration # {}. Train Loss:{:.2f}.Train Acc:{:.0f}%'.format(*acc_and_loss))\n",
    "            writer.add_summary(s,i)\n",
    "        if i%(EVAL_EVERY*20)==0:\n",
    "            train_confusion_matrix=sess.run([confusion_matrix],feed_dict={x:X, y:Y,dropout:0.5})\n",
    "            header=LABEL_TO_INDEX_MAP.keys()\n",
    "            print(header)\n",
    "            df=pd.DataFrame(np.reshape(train_confusion_matrix,(NUM_LABELS,NUM_LABELS)),index=header)\n",
    "            print('\\nCOnfusion matrix:\\n {}\\n'.format(df))\n",
    "            saver.save(sess,os.path.join(LOGDIR,'model.ckpt'),i)\n",
    "\n",
    "        sess.run(train_step,feed_dict={x:X,y:Y,dropout:0.5})\n",
    "    print('\\ntotal trinnig time: {:0f} seconds\\n'.format(time.time()-start_time))\n",
    "\n",
    "\n",
    "    #testing\n",
    "    batch= get_batch(BATCH_SIZE,PATH_TEST)\n",
    "    total_accuracy =0\n",
    "    for i in range(ITERATIONS_TEST):\n",
    "       # sess.run( tf.initialize_all_variables() )\n",
    "        X,Y = next(batch,PATH_TEST)\n",
    "        test_accuracy, s=sess.run([accuracy,summ],feed_dict={x:X,y:Y,dropout:1.0})\n",
    "        print('iteration # {}. TEst Accuracy: {:.0f}%'.format(i+1,test_accuracy*100))\n",
    "        total_accuracy+=(test_accuracy/ITERATIONS_TEST)\n",
    "        test_writer.add_summary(s,i)\n",
    "\n",
    "    print('\\n final accuracy : {:.0f}%'.format(total_accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start traininig\n",
      "\n",
      "iteration # 5. Train Loss:2.27.Train Acc:19%\n",
      "iteration # 10. Train Loss:2.02.Train Acc:18%\n",
      "iteration # 15. Train Loss:1.77.Train Acc:25%\n",
      "iteration # 20. Train Loss:1.73.Train Acc:27%\n",
      "iteration # 25. Train Loss:1.66.Train Acc:40%\n",
      "iteration # 30. Train Loss:1.69.Train Acc:30%\n",
      "iteration # 35. Train Loss:1.62.Train Acc:36%\n",
      "iteration # 40. Train Loss:1.54.Train Acc:44%\n",
      "iteration # 45. Train Loss:1.40.Train Acc:51%\n",
      "iteration # 50. Train Loss:1.46.Train Acc:39%\n",
      "iteration # 55. Train Loss:1.41.Train Acc:42%\n",
      "iteration # 60. Train Loss:1.33.Train Acc:52%\n",
      "iteration # 65. Train Loss:1.20.Train Acc:54%\n",
      "iteration # 70. Train Loss:1.14.Train Acc:58%\n",
      "iteration # 75. Train Loss:1.31.Train Acc:48%\n",
      "iteration # 80. Train Loss:1.16.Train Acc:60%\n",
      "iteration # 85. Train Loss:1.00.Train Acc:65%\n",
      "iteration # 90. Train Loss:1.04.Train Acc:60%\n",
      "iteration # 95. Train Loss:1.12.Train Acc:56%\n",
      "iteration # 100. Train Loss:1.06.Train Acc:56%\n",
      "dict_keys(['cat', 'tree', 'bird', 'dog', 'bed', 'house'])\n",
      "\n",
      "COnfusion matrix:\n",
      "        0  1  2  3   4   5\n",
      "cat    7  2  3  2   0   1\n",
      "tree   0  9  0  0   0   0\n",
      "bird   1  1  8  2   1   0\n",
      "dog    2  5  4  9   3   1\n",
      "bed    0  1  3  2  11   1\n",
      "house  2  0  1  0   0  18\n",
      "\n",
      "iteration # 105. Train Loss:0.90.Train Acc:63%\n",
      "iteration # 110. Train Loss:0.96.Train Acc:65%\n",
      "iteration # 115. Train Loss:0.83.Train Acc:72%\n",
      "iteration # 120. Train Loss:0.93.Train Acc:66%\n",
      "iteration # 125. Train Loss:0.73.Train Acc:74%\n",
      "iteration # 130. Train Loss:0.67.Train Acc:79%\n",
      "iteration # 135. Train Loss:0.78.Train Acc:66%\n",
      "iteration # 140. Train Loss:0.73.Train Acc:72%\n",
      "iteration # 145. Train Loss:0.59.Train Acc:78%\n",
      "iteration # 150. Train Loss:0.81.Train Acc:68%\n",
      "iteration # 155. Train Loss:0.82.Train Acc:72%\n",
      "iteration # 160. Train Loss:0.62.Train Acc:76%\n",
      "iteration # 165. Train Loss:0.69.Train Acc:73%\n",
      "iteration # 170. Train Loss:0.68.Train Acc:76%\n",
      "iteration # 175. Train Loss:0.83.Train Acc:74%\n",
      "iteration # 180. Train Loss:0.66.Train Acc:77%\n",
      "iteration # 185. Train Loss:0.62.Train Acc:80%\n",
      "iteration # 190. Train Loss:0.59.Train Acc:77%\n",
      "iteration # 195. Train Loss:0.62.Train Acc:76%\n",
      "iteration # 200. Train Loss:0.63.Train Acc:77%\n",
      "dict_keys(['cat', 'tree', 'bird', 'dog', 'bed', 'house'])\n",
      "\n",
      "COnfusion matrix:\n",
      "         0   1   2   3   4   5\n",
      "cat    11   0   2   1   0   7\n",
      "tree    0  11   0   2   0   3\n",
      "bird    1   0  16   1   1   0\n",
      "dog     0   0   0  17   0   0\n",
      "bed     0   0   2   0  10   0\n",
      "house   0   0   2   0   0  13\n",
      "\n",
      "iteration # 205. Train Loss:0.60.Train Acc:78%\n",
      "iteration # 210. Train Loss:0.72.Train Acc:74%\n",
      "iteration # 215. Train Loss:0.82.Train Acc:74%\n",
      "iteration # 220. Train Loss:0.71.Train Acc:80%\n",
      "iteration # 225. Train Loss:0.71.Train Acc:78%\n",
      "iteration # 230. Train Loss:0.66.Train Acc:78%\n",
      "iteration # 235. Train Loss:0.63.Train Acc:78%\n",
      "iteration # 240. Train Loss:0.57.Train Acc:82%\n",
      "iteration # 245. Train Loss:0.55.Train Acc:79%\n",
      "iteration # 250. Train Loss:0.63.Train Acc:84%\n",
      "iteration # 255. Train Loss:0.56.Train Acc:78%\n",
      "iteration # 260. Train Loss:0.58.Train Acc:80%\n",
      "iteration # 265. Train Loss:0.64.Train Acc:75%\n",
      "iteration # 270. Train Loss:0.64.Train Acc:83%\n",
      "iteration # 275. Train Loss:0.46.Train Acc:86%\n",
      "iteration # 280. Train Loss:0.52.Train Acc:81%\n",
      "iteration # 285. Train Loss:0.50.Train Acc:84%\n",
      "iteration # 290. Train Loss:0.50.Train Acc:86%\n",
      "iteration # 295. Train Loss:0.46.Train Acc:84%\n",
      "iteration # 300. Train Loss:0.47.Train Acc:87%\n",
      "dict_keys(['cat', 'tree', 'bird', 'dog', 'bed', 'house'])\n",
      "\n",
      "COnfusion matrix:\n",
      "         0   1   2   3   4   5\n",
      "cat    10   1   1   0   1   0\n",
      "tree    0  11   0   1   1   0\n",
      "bird    0   0  15   0   0   0\n",
      "dog     0   0   1  20   0   0\n",
      "bed     0   0   1   0  18   0\n",
      "house   0   1   1   0   0  17\n",
      "\n",
      "iteration # 305. Train Loss:0.56.Train Acc:88%\n",
      "iteration # 310. Train Loss:0.44.Train Acc:88%\n",
      "iteration # 315. Train Loss:0.45.Train Acc:85%\n",
      "iteration # 320. Train Loss:0.44.Train Acc:84%\n",
      "iteration # 325. Train Loss:0.51.Train Acc:83%\n",
      "iteration # 330. Train Loss:0.47.Train Acc:85%\n",
      "iteration # 335. Train Loss:0.47.Train Acc:84%\n",
      "iteration # 340. Train Loss:0.56.Train Acc:82%\n",
      "iteration # 345. Train Loss:0.66.Train Acc:77%\n",
      "iteration # 350. Train Loss:0.27.Train Acc:88%\n",
      "iteration # 355. Train Loss:0.57.Train Acc:77%\n",
      "iteration # 360. Train Loss:0.45.Train Acc:85%\n",
      "iteration # 365. Train Loss:0.39.Train Acc:87%\n",
      "iteration # 370. Train Loss:0.44.Train Acc:88%\n",
      "iteration # 375. Train Loss:0.41.Train Acc:87%\n",
      "iteration # 380. Train Loss:0.37.Train Acc:89%\n",
      "iteration # 385. Train Loss:0.33.Train Acc:95%\n",
      "iteration # 390. Train Loss:0.41.Train Acc:85%\n",
      "iteration # 395. Train Loss:0.64.Train Acc:76%\n",
      "iteration # 400. Train Loss:0.41.Train Acc:86%\n",
      "dict_keys(['cat', 'tree', 'bird', 'dog', 'bed', 'house'])\n",
      "\n",
      "COnfusion matrix:\n",
      "         0   1   2   3   4   5\n",
      "cat    12   2   2   0   0   2\n",
      "tree    1  18   0   1   0   0\n",
      "bird    0   0  15   1   0   1\n",
      "dog     0   1   1  11   1   0\n",
      "bed     0   0   0   0  13   0\n",
      "house   0   0   0   0   0  18\n",
      "\n",
      "iteration # 405. Train Loss:0.47.Train Acc:83%\n",
      "iteration # 410. Train Loss:0.46.Train Acc:84%\n",
      "iteration # 415. Train Loss:0.37.Train Acc:89%\n",
      "iteration # 420. Train Loss:0.29.Train Acc:93%\n",
      "iteration # 425. Train Loss:0.47.Train Acc:88%\n",
      "iteration # 430. Train Loss:0.30.Train Acc:91%\n",
      "iteration # 435. Train Loss:0.36.Train Acc:87%\n",
      "iteration # 440. Train Loss:0.40.Train Acc:85%\n",
      "iteration # 445. Train Loss:0.29.Train Acc:91%\n",
      "iteration # 450. Train Loss:0.38.Train Acc:88%\n",
      "iteration # 455. Train Loss:0.31.Train Acc:90%\n",
      "iteration # 460. Train Loss:0.36.Train Acc:87%\n",
      "iteration # 465. Train Loss:0.33.Train Acc:87%\n",
      "iteration # 470. Train Loss:0.42.Train Acc:86%\n",
      "iteration # 475. Train Loss:0.34.Train Acc:89%\n",
      "iteration # 480. Train Loss:0.26.Train Acc:90%\n",
      "iteration # 485. Train Loss:0.49.Train Acc:83%\n",
      "iteration # 490. Train Loss:0.26.Train Acc:92%\n",
      "iteration # 495. Train Loss:0.33.Train Acc:90%\n",
      "iteration # 500. Train Loss:0.27.Train Acc:91%\n",
      "dict_keys(['cat', 'tree', 'bird', 'dog', 'bed', 'house'])\n",
      "\n",
      "COnfusion matrix:\n",
      "        0   1   2   3   4   5\n",
      "cat    9   1   0   0   0   0\n",
      "tree   2  15   0   0   0   0\n",
      "bird   2   0  19   0   1   0\n",
      "dog    1   0   0  20   0   0\n",
      "bed    0   0   0   0  15   0\n",
      "house  1   0   0   0   0  14\n",
      "\n",
      "\n",
      "total trinnig time: 2413.436576 seconds\n",
      "\n",
      "iteration # 1. TEst Accuracy: 95%\n",
      "iteration # 2. TEst Accuracy: 92%\n",
      "iteration # 3. TEst Accuracy: 92%\n",
      "iteration # 4. TEst Accuracy: 95%\n",
      "iteration # 5. TEst Accuracy: 89%\n",
      "iteration # 6. TEst Accuracy: 90%\n",
      "iteration # 7. TEst Accuracy: 91%\n",
      "iteration # 8. TEst Accuracy: 91%\n",
      "iteration # 9. TEst Accuracy: 93%\n",
      "iteration # 10. TEst Accuracy: 94%\n",
      "\n",
      " final accuracy : 92%\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    init(PATH_TRAIN)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
